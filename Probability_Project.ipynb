{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff2ec1eb-24d0-4e15-8e63-305d3019c756",
   "metadata": {},
   "source": [
    "# SPAM CLASSIFIER \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b12765-742d-4c4b-aae2-14b4838963d6",
   "metadata": {},
   "source": [
    "### INTRODUCTION\n",
    "1) This is a spam classifier which takes a SMS and predicts whether it is a spam or a ham.\n",
    "2) The libraries used in the making of the project are mentioned below \n",
    "3) Short explanation of the code below:\n",
    "    * Created a function \"processData\" to process the data that is to remove punctuations,convert to lowercase and tokenize the data.\n",
    "    * Separate the data into SMStrainingSet and SMStestSet using a function SMSseparate and store both into arrays\n",
    "           for example : [ ['spam', [words in it] ] ....]\n",
    "    * We created a makeSMSvocab which stores all the words in a dictionary as keys and in values we have how many times the word appeared in a ham or spam which we maintain using Counter\n",
    "    * We used the Binary method and created functions for prior( P( C ) ) , likelihood( P( wd | C ) ) , score\n",
    "    * We also define a fuction called predictor which takes processed Data as input and tells whether it is ham or spam.\n",
    "    * We then have a function which makes the confusion matrix as well as tells us the accuracy of our model\n",
    "    * At last we have a function which tells the top 10 words for each class( ham or spam )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b390c40-84d3-4ae1-9f9b-344871715792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4a53c5-e540-4721-9844-a58ab4d07b19",
   "metadata": {},
   "source": [
    "### IMPORTING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39c2e53-66dc-41ab-9af8-4c2ac3d6c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing the data using pandas read function\n",
    "df = pd.read_csv(\"SMSSpamCollection\", sep=\"\\t\", names=[\"label\", \"text\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f098ff0-9b79-416b-b00f-b6665a18000f",
   "metadata": {},
   "source": [
    "### FUNCTIONS TO PROCESS THE DATA\n",
    "    1) removePunc(text):\n",
    "         takes a string as input and removes all numbers and punctuations from it returns the string\n",
    "    2) lowerCase(text):\n",
    "        takes a string as input and make all alphabets to lower case and returns the string\n",
    "    3) tokenize(text):\n",
    "        takes a string as input and separates all the words and store them in a array and returns the array   \n",
    "    4) processData(text):\n",
    "        takes string as input and uses all three above function to process the data (easier to use than using three functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa9abff-bb3a-4fce-b3b0-255fdb436b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunc(text):\n",
    "    newText = \"\"\n",
    "    punc = string.punctuation  #punc contains all the punctuations and numbers which need to be removed from data\n",
    "    punc += \"£0123456789\"\n",
    "    for i in text:\n",
    "        if i == \"\\n\": # this removes the nextline and adds space in place of the that new line thus , taking care of newline characters\n",
    "            i = \" \"\n",
    "        if i not in punc:  # if the character is not in punc then it is added to the newText which is returned , thus removing undesired characters\n",
    "            newText += i\n",
    "    return newText\n",
    "\n",
    "\n",
    "def lowerCase(text):\n",
    "    return text.lower()  # in built function which converts text to lower case #\n",
    "\n",
    "'''\n",
    "what we do here is we create 'x' in which we keep storing the characters until a space appears at that time if x is not empty\n",
    "then we append it in the 'arr' else we do nothing and keep going until another character appears . This way we separate the\n",
    "all the words and store them in the 'arr'\n",
    "'''\n",
    "def tokenize(text):\n",
    "    arr = []\n",
    "    x = \"\"\n",
    "    for i in text: \n",
    "        if i != \" \": \n",
    "            x += i\n",
    "        elif i == \" \" and len(x) > 0:\n",
    "            arr.append(x)\n",
    "            x = \"\"\n",
    "        else:\n",
    "            continue\n",
    "    if len(x) > 0:\n",
    "        arr.append(x)\n",
    "\n",
    "    return arr\n",
    "\n",
    "# In this we use all the above three functions to make it easier to process the data later by calling only one function\n",
    "def processData(text):  \n",
    "    text = removePunc(text)\n",
    "    text = lowerCase(text)\n",
    "    text = tokenize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fc5c1c-908c-4a0c-92f3-b8b176b96282",
   "metadata": {},
   "source": [
    "### PROCESSING THE DATA\n",
    "    We process the data here and store it in procData in the form : [['ham',['i','am',..]],['spam',['free','yes'..]]....]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f08d8f-3a41-440d-8ade-1eb01641929a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "using the 'loc' function of pandas where 'text' specifies the column and 'i' tells the row number so we get the data from the \n",
    "ith row and column 'text' which is nothing but SMS\n",
    "'''a\n",
    "procData = []\n",
    "for i in range(len(df)):\n",
    "    x = df.loc[i, \"text\"] \n",
    "    x = processData(x) # after retreiving the data it is processed using 'processData'\n",
    "    procData.append([df.loc[i, \"label\"], x])  # here it is appended in the 'procData' in the form ['ham/spam',x(processedData)]\n",
    "print(len(procData))\n",
    "procData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7996a7-e04e-4f89-9f6e-b170e7e6d830",
   "metadata": {},
   "source": [
    "### SEPARATING THE DATA INTO TESTSET AND TRAINING SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a5dd88-3009-4cae-8092-8024f5d95982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SMStrainingSet = []\n",
    "SMScount = {\"ham\": 0, \"spam\": 0}  # Here we save the count of ham documents and count of spam documents in the training Set\n",
    "SMStestSet = []\n",
    "'''\n",
    "The procData is separated into training set and test set using the SMSseparateData which takes the percent of data that should be \n",
    "in the the training set and uses 'split' which tells the amout of files that should be in the training set and we separated them \n",
    "by using 'cnt' and using if else conditions .\n",
    "'''\n",
    "def SMSseparateTheData(\n",
    "    DataInTraining,\n",
    "):  # please give percentage of data that should be in trainingSet\n",
    "    split = round((DataInTraining * len(procData)) / 100) # amout of files that should be in training set \n",
    "    cnt = 0\n",
    "    for i in procData:\n",
    "        cnt += 1\n",
    "        if cnt <= split: # while cnt<=split put into training set\n",
    "            SMScount[i[0]] += 1 # counting ham or spam in training set\n",
    "            SMStrainingSet.append(i)\n",
    "        else:\n",
    "            SMStestSet.append(i) # while cnt>split put into test set\n",
    "\n",
    "\n",
    "SMSseparateTheData(89.75)\n",
    "len(SMStrainingSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8d7fa8-088c-4973-a34f-9199bd04c540",
   "metadata": {},
   "source": [
    "### DEFINING VOCABULARY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d27462-b769-4f7b-a09e-61da5669c016",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SMSV = {}\n",
    "\n",
    "'''\n",
    "Here we create the vocabulary of the training set and store them in 'SMSV' dictionary which contains words as keys and a Counter\n",
    "dictionary which saves in how many spams or hams have the words appeared in as value .\n",
    "'''\n",
    "def makeSMSvocab():\n",
    "    for i in SMStrainingSet:\n",
    "        uniqueWordsInSMS = set(i[1]) # we use set to remove duplicates and keep only unique words\n",
    "        d = Counter() # Counter dictionary which stores in how many spams and hams have the word appeared in\n",
    "        if i[0] == \"spam\":\n",
    "            d[\"spam\"] = 1\n",
    "        else:\n",
    "            d[\"ham\"] = 1\n",
    "        for j in uniqueWordsInSMS:\n",
    "            if j not in SMSV:\n",
    "                SMSV[j] = d.copy() # if the word is not in dictionary we put it there\n",
    "            else:\n",
    "                SMSV[j] += d # if it is already there we add the 'd' to that word\n",
    "\n",
    "\n",
    "makeSMSvocab()\n",
    "SMSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8383258-ecff-40d4-a9e8-b60621a0ac1d",
   "metadata": {},
   "source": [
    "## FUNCTIONS TO COMPUTE PRIOR , LIKELIHOOD AND SCORE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3434f07-ed19-42a3-88ff-60cc0da89d63",
   "metadata": {},
   "source": [
    "### FUCNTION FOR PRIOR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c926b30-d514-4f3f-9902-5ad8d8d99875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cls=class\n",
    "'''\n",
    "The prior prabability of a class is nothing but total count of documents that belong to that classin training set divided  by \n",
    "the total documents in training Set\n",
    "'''\n",
    "def SMScomputePrior(cls):\n",
    "    return SMScount[cls] / len(SMStrainingSet)\n",
    "\n",
    "\n",
    "print(SMScomputePrior(\"spam\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09357d59-3c85-4a02-97d3-65363ea32622",
   "metadata": {},
   "source": [
    "### FUNCTION FOR LIKELIHOOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b80e2-6c99-4568-b480-e3086ec3e0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMSalpha = 0.00001  # smoothing parameter\n",
    "\n",
    "'''\n",
    "This model only considers whether a word is present or absent, not how many times it occurs. So the single word likelihood is \n",
    "nothing but P(w | C) = (# documents in class C containing  w+ alpha)/ (# documents in class C + 2α) which we can easily calculate\n",
    "using vocabulary 'SMSV' \n",
    "'''\n",
    "def SMSsingleWordLikelihood(w, cls):\n",
    "    if w in SMSV:\n",
    "        return (SMSV[w][cls] + SMSalpha) / (SMScount[cls] + 2 * SMSalpha) # if the word is present in the vocabulary\n",
    "    else:\n",
    "        return SMSalpha / (SMScount[cls] + 2 * SMSalpha) # if the word in not present in the vocabulary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def SMSlikelihood(wd, cls):\n",
    "    def indicator(wd, w):\n",
    "        if w in wd:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    totalProb = 1\n",
    "    for i in SMSV:\n",
    "        wordlikelihood = SMSsingleWordLikelihood(i, cls)\n",
    "        totalProb = (\n",
    "            totalProb\n",
    "            * ((wordlikelihood) ** (indicator(wd, i)))\n",
    "            * ((1 - wordlikelihood) ** (1 - indicator(wd, i)))\n",
    "        )\n",
    "    return totalProb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375c678b-1b67-4188-9491-cf50f388d512",
   "metadata": {},
   "source": [
    "### FUNCTION FOR SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa73dfe-a64d-4b39-83bb-4fad6bfbd816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the posterior scores\n",
    "def SMSposScores(wd, cls):\n",
    "    score = math.log(SMScomputePrior(cls))\n",
    "    for i in wd:\n",
    "        score += math.log(SMSsingleWordLikelihood(i, cls))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b2b8f5-7603-4dde-94c9-86435cbfea40",
   "metadata": {},
   "source": [
    "## PREDICTOR FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec9a5c5-a57f-4670-8b6b-5b583317a13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMSpredict(wd):\n",
    "    if SMSposScores(wd, \"ham\") > SMSposScores(wd, \"spam\"):\n",
    "        return \"ham\"\n",
    "    else:\n",
    "        return \"spam\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5888103d-3e5b-4207-94a8-b7bde05d9bdd",
   "metadata": {},
   "source": [
    "## CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedb3711-d584-49c9-ace6-9148b31fb049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMSconfusionMatrix(testCases):  # as a list like [['ham',SMS],['spam',SMS]....]\n",
    "    d = {}\n",
    "    for i in [\"spam\", \"ham\"]:\n",
    "        for j in [\"spam\", \"ham\"]:\n",
    "            x = i + \"-\" + j\n",
    "            d[x] = 0\n",
    "    for i in testCases:\n",
    "        prediction = SMSpredict(i[1])\n",
    "        d[i[0] + \"-\" + prediction] += 1\n",
    "\n",
    "    print(\"      \", \"spam\", \"    \", \"ham\")\n",
    "    print(\"---------------------------\")\n",
    "    print(\"spam\", \"  \", d[\"spam-spam\"], \"     \", d[\"spam-ham\"])\n",
    "    print(\"---------------------------\")\n",
    "    print(\"ham\", \"   \", d[\"ham-spam\"], \"     \", d[\"ham-ham\"])\n",
    "\n",
    "    print(\"---------------------------\")\n",
    "    print(\n",
    "        \"Accuracy is :\", ((d[\"spam-spam\"] + d[\"ham-ham\"]) / len(testCases)) * 100, \"%\"\n",
    "    )\n",
    "\n",
    "\n",
    "SMSconfusionMatrix(SMStestSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd382417-59f3-458c-b78a-7dc63eb388f2",
   "metadata": {},
   "source": [
    "## A FEW TEST CASES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d042ac5-f49b-473e-ae5c-e4eb9d184935",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfTestCases = [\n",
    "    [\"spam\", \"Win free tickets now!!!\"],\n",
    "    [\"ham\", \"Are you coming to the meeting?\"],\n",
    "    [\"spam\", \"URGENT! You won $1000\"],\n",
    "    [\"ham\", \"See you tomorrow at lunch\"],\n",
    "]\n",
    "\n",
    "for i in pdfTestCases:\n",
    "    i[1] = processData(i[1])\n",
    "\n",
    "SMSconfusionMatrix(pdfTestCases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3f6c77-4559-47b0-9ab6-3586428bf1d4",
   "metadata": {},
   "source": [
    "## TOP 10 MOST INDICATIVE WORDS FOR EACH CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e903923e-ce01-4da0-89d3-387a9c27934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 most indicative words for each class\n",
    "def topSMSwords():\n",
    "    arr = []\n",
    "    for i in SMSV:\n",
    "        x = SMSposScores([i], \"ham\") - SMSposScores([i], \"spam\")\n",
    "        arr.append([x, i])\n",
    "    arr.sort()\n",
    "    cnt = 1\n",
    "    print(\"Top 10 most Indicative words for Spam : \")\n",
    "    for i in range(0, 10):\n",
    "        print(i + 1, \")\", arr[i][1])\n",
    "\n",
    "    print()\n",
    "    print(\"Top 10 most Indicative words for Ham : \")\n",
    "    for i in range(len(arr) - 10, len(arr)):\n",
    "        print(i - len(arr) + 11, \")\", arr[i][1])\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "topSMSwords()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f938ee2a-8f34-43d7-be4f-ef54f65ec3d1",
   "metadata": {},
   "source": [
    "# BBC CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400e399e-3166-49a3-9d2a-9c8f50936f2e",
   "metadata": {},
   "source": [
    "## INTRODUCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aef9deb-0186-4d88-ad5d-07e40765b7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259a93df-18a7-4da6-b06a-46445d455327",
   "metadata": {},
   "source": [
    "## SEPARATING INTO TRAINING AND TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de588b5c-9c6b-4f85-ba9e-f2625179447b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "basePath = \"News Articles\"\n",
    "\n",
    "cntClasses = {}\n",
    "\n",
    "BBCtrainingSet = {}\n",
    "BBCtestSet = []\n",
    "\n",
    "\n",
    "def BBCseparate(\n",
    "    DataInTraining,\n",
    "):  # please give percentage and this will be applied to all the classes meaning from every class this much percentage of data will be taken\n",
    "\n",
    "    for i in os.listdir(basePath):\n",
    "        classPath = os.path.join(basePath, i)\n",
    "\n",
    "        cntClasses[i] = 0\n",
    "        cnt = 0\n",
    "\n",
    "        size = len(os.listdir(classPath))\n",
    "\n",
    "        split = round((DataInTraining * size) / 100)\n",
    "\n",
    "        cntClasses[i] = split\n",
    "        for j in os.listdir(classPath):\n",
    "            filePath = os.path.join(classPath, j)\n",
    "\n",
    "            with open(filePath, \"r\") as f:\n",
    "                cnt += 1\n",
    "                content = f.read()\n",
    "                content = processData(content)\n",
    "                d = Counter(content)\n",
    "                if cnt <= split:\n",
    "                    if i in BBCtrainingSet:\n",
    "                        BBCtrainingSet[i] += d\n",
    "                    else:\n",
    "                        BBCtrainingSet[i] = d\n",
    "                else:\n",
    "                    BBCtestSet.append([i, content])\n",
    "\n",
    "\n",
    "BBCseparate(80)\n",
    "BBCtrainingSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786d0f5b-1679-4b1c-8e9d-4eb77e4fe1ea",
   "metadata": {},
   "source": [
    "## DEFINING VOCABULARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6464077e-2025-4ba9-af59-5ab617430f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BBCV = Counter()\n",
    "\n",
    "\n",
    "def makeBBCvocab():\n",
    "    global BBCV\n",
    "    for i in BBCtrainingSet:\n",
    "        BBCV += BBCtrainingSet[i]\n",
    "    return\n",
    "\n",
    "\n",
    "makeBBCvocab()\n",
    "len(BBCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a78ce27-848a-4731-832f-84a76889857b",
   "metadata": {},
   "source": [
    "## CALCULATIN TOTAL WORDS IN EACH CLASS AND TOTAL DOCUMENTS IN TRAINING SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f42aca8-29d4-4b8d-aad1-f6772b8c4fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "totalwordsinCls = {}\n",
    "Totaldoc = 0\n",
    "\n",
    "\n",
    "def totalWordsInClsAndTotalDoc():\n",
    "    global Totaldoc\n",
    "    for i in BBCtrainingSet:\n",
    "        cnt = 0\n",
    "        for j in BBCtrainingSet[i]:\n",
    "            cnt += BBCtrainingSet[i][j]\n",
    "        totalwordsinCls[i] = cnt\n",
    "\n",
    "    for i in cntClasses:\n",
    "        Totaldoc += cntClasses[i]\n",
    "\n",
    "\n",
    "totalWordsInClsAndTotalDoc()\n",
    "totalwordsinCls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6880c8c-bf99-4210-9007-063566e4d34f",
   "metadata": {},
   "source": [
    "## FUNCTIONS TO COMPUTE PRIOR , LIKELIHOOD AND SCORE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a25577-1661-4a03-8131-26b75aec99db",
   "metadata": {},
   "source": [
    "### FUNCTION FOR PRIOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd84e26f-3c35-4f97-9b0e-532aa0bab4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BBCalpha = 0.000001\n",
    "\n",
    "\n",
    "def BBCpriorProbability(cls):\n",
    "    return cntClasses[cls] / Totaldoc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292f6f71-8009-4ad3-9e77-b97299c3a9f1",
   "metadata": {},
   "source": [
    "### FUNCTION FOR LIKELIHOOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c361e8-0429-4a4b-bdf1-a3f5d9d73314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BBCsingleWordLikelihood(w, cls):\n",
    "    return (BBCtrainingSet[cls][w] + BBCalpha) / (\n",
    "        totalwordsinCls[cls] + len(BBCV) * BBCalpha\n",
    "    )\n",
    "\n",
    "\n",
    "def BBClikelihood(wd, cls):\n",
    "    d = Counter(wd)\n",
    "    totalProb = 1\n",
    "    for i in wd:\n",
    "        totalProb = totalProb * (BBCsingleWordLikelihood(i, cls) ** (d[i]))\n",
    "    return totalProb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3dcf75-aac3-4e66-b929-a8ae574b0e96",
   "metadata": {},
   "source": [
    "### FUNCTION FOR SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ac2aa-13d1-4a16-b6e5-fcfcce4893e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BBCposScore(wd, cls):\n",
    "    score = math.log(BBCpriorProbability(cls))\n",
    "    for i in wd:\n",
    "        score += math.log(BBCsingleWordLikelihood(i, cls))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef35ed9-2266-43ce-9a45-a125415e81ef",
   "metadata": {},
   "source": [
    "## PREDICTOR FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fc30aa-4519-4f78-90d1-a501dbdd0542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BBCpredictor(wd):\n",
    "    maxi = float(\"-inf\")\n",
    "    category = \"\"\n",
    "    for i in BBCtrainingSet:\n",
    "        x = BBCposScore(wd, i)\n",
    "        if x > maxi:\n",
    "            maxi = x\n",
    "            category = i\n",
    "    return category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcee4e3-64dc-4418-b0d3-4463038bafc2",
   "metadata": {},
   "source": [
    "## CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42be8b74-e253-4dab-9541-7b401dae00f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BBCconfusionMatrix(testCases):\n",
    "    dic = {}\n",
    "    for i in BBCtrainingSet:\n",
    "        for j in BBCtrainingSet:\n",
    "            dic[i[0] + \"-\" + j[0]] = 0\n",
    "    for i in testCases:\n",
    "        ans = BBCpredictor(i[1])\n",
    "        dic[i[0][0] + \"-\" + ans[0]] += 1\n",
    "\n",
    "    print(\"                  \",\"business\",\"    \",\"entertainment\",\"    \",\"politics\",\"    \",\"sport\",\"      \",\"tech\")\n",
    "    print(\"-------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    print(\"business\",\"           \",dic[\"b-b\"],\"             \",dic[\"b-e\"],\"               \",dic[\"b-p\"],\"         \",dic[\"b-s\"],\"         \",dic[\"b-t\"])\n",
    "    print(\"-------------------------------------------------------------------------------------\")\n",
    "\n",
    "    print(\"entertainment\",\"      \",dic[\"e-b\"],\"              \",dic[\"e-e\"],\"              \",dic[\"e-p\"],\"         \",dic[\"e-s\"],\"         \",dic[\"e-t\"])\n",
    "    print(\"-------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    print(\"politics\",\"           \",dic[\"p-b\"],\"              \",dic[\"p-e\"],\"               \",dic[\"p-p\"],\"        \",dic[\"p-s\"],\"         \",dic[\"p-t\"])\n",
    "    print(\"-------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    print(\"sport\",\"              \",dic[\"s-b\"],\"              \",dic[\"s-e\"],\"               \",dic[\"s-p\"],\"         \",dic[\"s-s\"],\"        \",dic[\"s-t\"])\n",
    "    print(\"-------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    print(\"tech\",\"               \",dic[\"t-b\"],\"              \",dic[\"t-e\"],\"               \",dic[\"t-p\"],\"         \",dic[\"t-s\"],\"         \",dic[\"t-t\"])\n",
    "    print(\"-------------------------------------------------------------------------------------\")\n",
    "\n",
    "    acc = 0\n",
    "    for i in dic:\n",
    "        if i[0] == i[2] :\n",
    "            acc += dic[i]\n",
    "    print(\"Accuracy is :\" , ( acc/len(testCases))*100,\"%\")\n",
    "    return\n",
    "\n",
    "\n",
    "BBCconfusionMatrix(BBCtestSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca60146-f419-4b54-b871-203be0c5461e",
   "metadata": {},
   "source": [
    "## A FEW TEST CASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3246fa-b18e-4ebd-9942-825ba904e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfTestCases = [\n",
    "    [\"business\", \"Stock market crashes as oil prices rise\"],\n",
    "    [\"sport\", \"Premier League team wins the championship\"],\n",
    "    [\"politics\", \"Government passes new healthcare reform\"],\n",
    "    [\"tech\", \"Apple releases latest iPhone with new features\"],\n",
    "    [\"entertainment\", \"Celebrity announces new film project\"],\n",
    "]\n",
    "for i in pdfTestCases:\n",
    "    i[1] = processData(i[1])\n",
    "\n",
    "BBCconfusionMatrix(pdfTestCases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed388c4-b57c-4abf-82ad-f7e74648867f",
   "metadata": {},
   "source": [
    "## TOP 10 MOST INDICATIVE WORDS FOR EACH CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c118deab-0bf5-44b0-89c6-b2b5ad181768",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicative_words = {}\n",
    "\n",
    "\n",
    "def topBBCwords():\n",
    "    for c in BBCtrainingSet:\n",
    "        word_scores = []\n",
    "        for w in BBCV:\n",
    "            pw_c = math.log(BBCsingleWordLikelihood(w, c))\n",
    "            other_max = max(\n",
    "                math.log(BBCsingleWordLikelihood(w, other))\n",
    "                for other in BBCtrainingSet\n",
    "                if other != c\n",
    "            )\n",
    "            score = pw_c - other_max\n",
    "            word_scores.append((score, w))\n",
    "        word_scores.sort(reverse=True)\n",
    "        indicative_words[c] = [w for _, w in word_scores[:10]]\n",
    "\n",
    "    for c in indicative_words:\n",
    "        cnt = 1\n",
    "        print(f\"\\nTop words for {c}:\")\n",
    "        for w in indicative_words[c]:\n",
    "            print(cnt, \")\", w)\n",
    "            cnt += 1\n",
    "    return\n",
    "\n",
    "\n",
    "topBBCwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04aa1fa-d699-4122-8ab7-b03b928bbb6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
